{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd40f4f4-5a80-40b7-94b3-aed136daa9d8",
   "metadata": {},
   "source": [
    "# Process the Load Data for the NTP Heat Wave Grid Stress Events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e597e9-538f-41d4-afa9-a268b10ab1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the packages we need:\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f6fe5-0769-4298-887e-7c4cffc64a0d",
   "metadata": {},
   "source": [
    "## Set the Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd887600-32da-46dc-b411-fa63c75f564f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the data input and image output directory:\n",
    "data_dir =  '/Users/burl878/Documents/Code/code_repos/ntp_heat_wave_loads/data/'\n",
    "plot_dir =  '/Users/burl878/Documents/Code/code_repos/ntp_heat_wave_loads/plots/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb4d0d-a7cd-4dbf-aa21-c1b162873993",
   "metadata": {},
   "source": [
    "## Suppress Future Warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f4aa11-ca79-407d-a18d-4c29e63fed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78893b17-7c20-4ea5-8292-90dea6799307",
   "metadata": {},
   "source": [
    "## Create a Function to Process the 2035 GridView Data Used in Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99e5c4-6575-4d2e-843b-21064c017f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gridview_data(data_dir: str):\n",
    "    \n",
    "    # Read in the raw data .csv file:\n",
    "    gv_df = pd.read_csv((data_dir + 'wecc_load_2035.csv'))\n",
    "    \n",
    "    # Subset to just the annual total demand by BA:\n",
    "    gv_df = gv_df[-3:-2]\n",
    "       \n",
    "    # Strip the unecessary bits from the column names:\n",
    "    gv_df.columns = gv_df.columns.str.replace(\"_CEC\", \"\")\n",
    "    gv_df.columns = gv_df.columns.str.replace(\"_2030.dat\", \"\")\n",
    "    gv_df.columns = gv_df.columns.str.replace(\"Load_\", \"\")\n",
    "       \n",
    "    # Delete the index and last column:\n",
    "    del gv_df[\"Index\"], gv_df[\"Unnamed: 38\"]\n",
    "    \n",
    "    # Compute the total loads for CISO, IPCO, NEVP, and PACE:\n",
    "    gv_df['CISO'] = (gv_df['CIPB'] + gv_df['CIPV'] + gv_df['CISC'] + gv_df['CISD'] + gv_df['VEA']).round(2)\n",
    "    gv_df['IPCO'] = (gv_df['IPFE'] + gv_df['IPMV'] + gv_df['IPTV']).round(2)\n",
    "    gv_df['PACE'] = (gv_df['PAID'] + gv_df['PAUT'] + gv_df['PAWY']).round(2)\n",
    "    gv_df['NEVP_Sum'] = (gv_df['NEVP'] + gv_df['SPPC']).round(2)\n",
    "           \n",
    "    # Rename a few columns for consistency:\n",
    "    gv_df.rename(columns={'CIPB': 'CISO_CIPB', 'CIPV': 'CISO_CIPV', 'CISC': 'CISO_CISC', 'CISD': 'CISO_CISD', 'VEA': 'CISO_VEA',\n",
    "                          'IPFE': 'IPCO_IPFE', 'IPMV': 'IPCO_IPMV', 'IPTV': 'IPCO_IPTV',\n",
    "                          'NEVP': 'NEVP_NEVP', 'SPPC': 'NEVP_SPPC',\n",
    "                          'PAID': 'PACE_PAID', 'PAUT': 'PACE_PAUT', 'PAWY': 'PACE_PAWY'}, inplace=True) \n",
    "    gv_df.rename(columns={'NEVP_Sum': 'NEVP'}, inplace=True) \n",
    "    \n",
    "    # Squeeze the dataframe:\n",
    "    gv_df = gv_df.squeeze().to_frame()\n",
    "        \n",
    "    # Rename the columns:\n",
    "    gv_df.reset_index(inplace=True)\n",
    "    gv_df = gv_df.rename(columns = {'index':'BA'})\n",
    "    gv_df.rename(columns={gv_df.columns[1]: \"Total_Load_MWh\" }, inplace = True)\n",
    "       \n",
    "    # Sort the dataframe alphabetically by BA name:\n",
    "    gv_df = gv_df.sort_values('BA')\n",
    "       \n",
    "    # Return the output dataframe:\n",
    "    return gv_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9778e2e-41a0-4a77-b985-40a3cfaba372",
   "metadata": {},
   "source": [
    "## Create a Function to Aggregate the Raw TELL MLP Output into a Single Dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4dd356-95b4-43e8-987a-1012754a25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_mlp_output_files(data_dir: str, year_to_process: str):\n",
    "    \n",
    "    # Create a list of all of the MLP output files in the \"mlp_input_dir\" and aggregate the files in that list:\n",
    "    list_of_files = sorted(glob.glob(os.path.join(data_dir, 'TELL_Data', year_to_process, '*_mlp_output.csv')))\n",
    "\n",
    "    # Loop over the list of MLP output files:\n",
    "    for file in range(len(list_of_files)):\n",
    "\n",
    "        # Read in the .csv file and replace missing values with nan:\n",
    "        mlp_data = pd.read_csv(list_of_files[file]).replace(-9999, np.nan)\n",
    "\n",
    "        # Rename the \"Load\" variable:\n",
    "        mlp_data.rename(columns={'Load': 'Hourly_Load_MWh'}, inplace=True)\n",
    "\n",
    "        # Replacing missing or negative loads with NaN:\n",
    "        mlp_data.loc[~(mlp_data['Hourly_Load_MWh'] > 0), 'Hourly_Load_MWh'] = np.nan\n",
    "\n",
    "        # Aggregate the output into a new dataframe:\n",
    "        if file == 0:\n",
    "            tell_df = mlp_data\n",
    "        else:\n",
    "            tell_df = pd.concat([tell_df, mlp_data])\n",
    "    \n",
    "    # Return the output dataframe:\n",
    "    return tell_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c489f3c-66e8-4359-9e02-19057e5d87d3",
   "metadata": {},
   "source": [
    "## Create a Function to Scale the TELL Output Based on the GridView 2035 Values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412c8be-a4c5-4f58-8026-8ffd637ec7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_tell_loads(data_dir: str, year_to_process: str):\n",
    "    \n",
    "    # Aggregate the TELL MLP files:\n",
    "    tell_df = aggregate_mlp_output_files(data_dir = data_dir,\n",
    "                                         year_to_process = year_to_process)\n",
    "    \n",
    "    # Read in the processed GridView file and rename a column for consistency:\n",
    "    gv_df = process_gridview_data(data_dir = data_dir)\n",
    "    gv_df.rename(columns={'Total_Load_MWh': 'GV_Total_Load_MWh'}, inplace=True) \n",
    "    \n",
    "    # Merge the tell_df and gv_df dataframes based on common BA names:\n",
    "    merged_df = tell_df.merge(gv_df, on=['BA'])\n",
    "    \n",
    "    # Sum the hourly TELL loads by BA into annual total loads:\n",
    "    merged_df['TELL_Total_Load_MWh'] = merged_df.groupby('BA')['Hourly_Load_MWh'].transform('sum')\n",
    "    \n",
    "    # Compute the scaling factors that force the annual total loads to agree:\n",
    "    merged_df['Scaling_Factor'] = merged_df['GV_Total_Load_MWh'] / merged_df['TELL_Total_Load_MWh']\n",
    "    \n",
    "    # Compute the scaled hourly loads:\n",
    "    merged_df['Hourly_Load_MWh_Scaled'] = merged_df['Hourly_Load_MWh'] * merged_df['Scaling_Factor']\n",
    "    \n",
    "    # Compute the hours since the start of the year:\n",
    "    merged_df['Hour'] = ((pd.to_datetime(merged_df['Time_UTC']) - datetime.datetime(int(year_to_process), 1, 1, 0, 0, 0)) / np.timedelta64(1, 'h') + 1).astype(int)\n",
    "    \n",
    "    # Only keep the columns that are needed:\n",
    "    scaled_tell_df = merged_df[['Hour', 'BA', 'Hourly_Load_MWh_Scaled']].copy()\n",
    "    \n",
    "    # Drop the rows with missing values (i.e., there is not a corresponding GridView load):\n",
    "    scaled_tell_df = scaled_tell_df.dropna(how = 'any')\n",
    "    \n",
    "    # Rename the load variable and round it to 5 decimals:\n",
    "    scaled_tell_df.rename(columns={'Hourly_Load_MWh_Scaled': 'Load_MWh'}, inplace=True)\n",
    "    scaled_tell_df['Load_MWh'] = scaled_tell_df['Load_MWh'].round(5)\n",
    "    \n",
    "    # Return the output dataframe:\n",
    "    return scaled_tell_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db940a-bc5b-42ca-b1e6-2a97e5d7c009",
   "metadata": {},
   "source": [
    "## Create a Function to Format the Output for Ingest to GridView:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2d2d2-dadf-45cf-b0e1-40b8856595f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_scaled_tell_loads(data_dir: str, year_to_process: str):\n",
    "    \n",
    "    # Process the GridView file:\n",
    "    gv_df = process_gridview_data(data_dir = data_dir)\n",
    "    \n",
    "    # Compute the load fractions for the subregions:\n",
    "    CIPB_LF = (gv_df.loc[(gv_df['BA'] == 'CISO_CIPB')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    CIPV_LF = (gv_df.loc[(gv_df['BA'] == 'CISO_CIPV')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    CISC_LF = (gv_df.loc[(gv_df['BA'] == 'CISO_CISC')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    CISD_LF = (gv_df.loc[(gv_df['BA'] == 'CISO_CISD')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    VEA_LF  = (gv_df.loc[(gv_df['BA'] == 'CISO_VEA' )]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'CISO')]['Total_Load_MWh'].values[0])\n",
    "    IPFE_LF = (gv_df.loc[(gv_df['BA'] == 'IPCO_IPFE')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'IPCO')]['Total_Load_MWh'].values[0])\n",
    "    IPMV_LF = (gv_df.loc[(gv_df['BA'] == 'IPCO_IPMV')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'IPCO')]['Total_Load_MWh'].values[0])\n",
    "    IPTV_LF = (gv_df.loc[(gv_df['BA'] == 'IPCO_IPTV')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'IPCO')]['Total_Load_MWh'].values[0])\n",
    "    NEVP_LF = (gv_df.loc[(gv_df['BA'] == 'NEVP_NEVP')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'NEVP')]['Total_Load_MWh'].values[0])\n",
    "    SPPC_LF = (gv_df.loc[(gv_df['BA'] == 'NEVP_SPPC')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'NEVP')]['Total_Load_MWh'].values[0])\n",
    "    PAID_LF = (gv_df.loc[(gv_df['BA'] == 'PACE_PAID')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'PACE')]['Total_Load_MWh'].values[0])\n",
    "    PAUT_LF = (gv_df.loc[(gv_df['BA'] == 'PACE_PAUT')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'PACE')]['Total_Load_MWh'].values[0])\n",
    "    PAWY_LF = (gv_df.loc[(gv_df['BA'] == 'PACE_PAWY')]['Total_Load_MWh'].values[0]) / (gv_df.loc[(gv_df['BA'] == 'PACE')]['Total_Load_MWh'].values[0])\n",
    "    \n",
    "    # Aggregate the TELL MLP files:\n",
    "    scaled_tell_df = scale_tell_loads(data_dir = data_dir, \n",
    "                                      year_to_process = year_to_process)\n",
    "   \n",
    "    # Reshape the dataframe and drop the indexes:\n",
    "    load_df = scaled_tell_df.pivot(index = 'Hour', columns = 'BA', values = 'Load_MWh')\n",
    "    load_df = load_df.reset_index(drop=False)\n",
    "    \n",
    "    # Add back in the text to the column headers:\n",
    "    load_df = load_df.add_suffix('_2030.dat')\n",
    "    load_df = load_df.add_prefix('Load_')\n",
    "    \n",
    "    # Rename the time variable:\n",
    "    load_df.rename(columns={'Load_Hour_2030.dat': 'Index'}, inplace=True)\n",
    "    \n",
    "    # Compute the loads for the subregions:\n",
    "    load_df['Load_CIPB_2030_CEC.dat'] = load_df['Load_CISO_2030.dat'] * CIPB_LF\n",
    "    load_df['Load_CIPV_2030_CEC.dat'] = load_df['Load_CISO_2030.dat'] * CIPV_LF\n",
    "    load_df['Load_CISC_2030_CEC.dat'] = load_df['Load_CISO_2030.dat'] * CISC_LF\n",
    "    load_df['Load_CISD_2030_CEC.dat'] = load_df['Load_CISO_2030.dat'] * CISD_LF\n",
    "    load_df['Load_VEA_2030.dat'] = load_df['Load_CISO_2030.dat'] * VEA_LF\n",
    "    load_df['Load_IPFE_2030.dat'] = load_df['Load_IPCO_2030.dat'] * IPFE_LF\n",
    "    load_df['Load_IPMV_2030.dat'] = load_df['Load_IPCO_2030.dat'] * IPMV_LF\n",
    "    load_df['Load_IPTV_2030.dat'] = load_df['Load_IPCO_2030.dat'] * IPTV_LF\n",
    "    load_df['Load_NEVP_Temp_2030.dat'] = load_df['Load_NEVP_2030.dat'] * NEVP_LF\n",
    "    load_df['Load_SPPC_2030.dat'] = load_df['Load_NEVP_2030.dat'] * SPPC_LF\n",
    "    load_df['Load_PAID_2030.dat'] = load_df['Load_PACE_2030.dat'] * PAID_LF\n",
    "    load_df['Load_PAUT_2030.dat'] = load_df['Load_PACE_2030.dat'] * PAUT_LF\n",
    "    load_df['Load_PAWY_2030.dat'] = load_df['Load_PACE_2030.dat'] * PAWY_LF\n",
    "    \n",
    "    # Drop the un-needed columns for BAs with subregions:\n",
    "    del load_df['Load_NEVP_2030.dat'], load_df['Load_CISO_2030.dat'], load_df['Load_IPCO_2030.dat'], load_df['Load_PACE_2030.dat']\n",
    "    \n",
    "    # Clean up the NEVP naming:\n",
    "    load_df.rename(columns={'Load_NEVP_Temp_2030.dat': 'Load_NEVP_2030.dat'}, inplace=True)\n",
    "    \n",
    "    # Create a target dataframe with the spare hours:\n",
    "    target_df = pd.DataFrame({\"Index\": np.arange(1,8791,1)})\n",
    "    \n",
    "    # Merge load dataframe with the target dataframe:\n",
    "    merged_df = target_df.merge(load_df, on=['Index'], how='left')\n",
    "    \n",
    "    # Compute the summary statistics:\n",
    "    stats_df = merged_df.apply(['mean','sum','max','min'])\n",
    "    \n",
    "    # Fix the summary statistic labels:\n",
    "    stats_df.iloc[0, 0] = 'AVG'\n",
    "    stats_df.iloc[1, 0] = 'SUM'\n",
    "    stats_df.iloc[2, 0] = 'MAX'\n",
    "    stats_df.iloc[3, 0] = 'MIN'\n",
    "      \n",
    "    # Sort the data by column name and make the Index column appear first:\n",
    "    merged_df.rename(columns={'Index': 'AA'}, inplace=True)\n",
    "    merged_df = merged_df.sort_index(axis = 1)\n",
    "    merged_df.rename(columns={'AA': 'Index'}, inplace=True)\n",
    "    \n",
    "    # Add in a blank row and fill it with the year placeholder:\n",
    "    merged_df.loc[-0.5] = 0\n",
    "    merged_df = merged_df.sort_index().reset_index(drop=True)\n",
    "    merged_df.iloc[0, :] = '2030'\n",
    "    merged_df.at[0, 'Index'] = 'Year'\n",
    "        \n",
    "    # Merge the hourly load data and statistics dataframes together:\n",
    "    output_df = pd.concat([merged_df, stats_df], axis=0)\n",
    "    \n",
    "    # Replace NaNs with blank values:\n",
    "    output_df.replace(np.nan, \"\", regex=True)\n",
    "    \n",
    "    # Set the output filename:\n",
    "    if year_to_process == '2055':\n",
    "       output_filename = 'TELL_Loads_2035_Based_on_2015_Weather_With_Climate_Change.csv'\n",
    "    if year_to_process == '2058':\n",
    "       output_filename = 'TELL_Loads_2035_Based_on_2018_Weather_With_Climate_Change.csv'\n",
    "    \n",
    "    # Write out the dataframe to a .csv file:\n",
    "    output_df.to_csv((os.path.join(data_dir, output_filename)), sep=',', index=False)\n",
    "    \n",
    "    # Return the output dataframe:\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98b6ea-1ed0-4135-a547-4bbe941202f3",
   "metadata": {},
   "source": [
    "## Call the Necessary Functions to Process the Data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfdc9dd-cac3-4091-a81b-2232bf4b083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = format_scaled_tell_loads(data_dir = data_dir,\n",
    "                                     year_to_process = '2058')\n",
    "\n",
    "output_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a2b9d-6f4f-414d-b826-fb88975e819b",
   "metadata": {},
   "source": [
    "## Create a Function to Create Summary Plots of the Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b83b7-b578-406c-bc98-c495ba2e833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_plots(data_dir: str, year_to_process: str, image_output_dir: str, image_resolution: int, save_images=False):\n",
    "    \n",
    "    # Read in the raw data .csv file:\n",
    "    gv_df = pd.read_csv((data_dir + 'wecc_load_2035.csv'))\n",
    "    \n",
    "    # Strip the unecessary bits from the column names:\n",
    "    gv_df.columns = gv_df.columns.str.replace(\"_CEC\", \"\")\n",
    "    gv_df.columns = gv_df.columns.str.replace(\"_2030.dat\", \"\")\n",
    "    gv_df.columns = gv_df.columns.str.replace(\"Load_\", \"\")\n",
    "    \n",
    "    # Delete the random column:\n",
    "    del gv_df[\"Unnamed: 38\"]\n",
    "    \n",
    "    # Subset to just the annual total demand by BA:\n",
    "    gv_stats = gv_df[8791:8795]\n",
    "    \n",
    "    # Reshape the dataframe and rename the index column:\n",
    "    gv_stats_df = pd.melt(gv_stats, id_vars='Index', var_name='BA', value_name='GV_Value')\n",
    "    gv_stats_df.rename(columns={'Index': 'Statistic'}, inplace=True)\n",
    "    \n",
    "    # Drop the first row and last 4 rows:\n",
    "    gv_df = gv_df.tail(-1).iloc[:-4, :]\n",
    "    \n",
    "    # Compute the hourly total load across BAs:\n",
    "    gv_df['GV_Total_Load_MWh'] = gv_df.sum(axis=1)\n",
    "    \n",
    "    # Copy the total load to a new dataframe:\n",
    "    gv_load_df = gv_df[['Index', 'GV_Total_Load_MWh']].copy().iloc[0:8760, :]\n",
    "    gv_load_df.rename(columns={'Index': 'Hour'}, inplace=True)\n",
    "    \n",
    "    # Set the output filename:\n",
    "    if year_to_process == '2055':\n",
    "       output_filename = 'TELL_Loads_2035_Based_on_2015_Weather_With_Climate_Change.csv'\n",
    "    if year_to_process == '2058':\n",
    "       output_filename = 'TELL_Loads_2035_Based_on_2018_Weather_With_Climate_Change.csv'\n",
    "    \n",
    "    # Read in the raw TELL data .csv file:\n",
    "    tell_df = pd.read_csv((data_dir + output_filename))\n",
    "    \n",
    "    # Strip the unecessary bits from the column names:\n",
    "    tell_df.columns = tell_df.columns.str.replace(\"_CEC\", \"\")\n",
    "    tell_df.columns = tell_df.columns.str.replace(\"_2030.dat\", \"\")\n",
    "    tell_df.columns = tell_df.columns.str.replace(\"Load_\", \"\")\n",
    "    \n",
    "    # Subset to just the annual total demand by BA:\n",
    "    tell_stats = tell_df[8791:8795]\n",
    "    \n",
    "    # Reshape the dataframe and rename the index column:\n",
    "    tell_stats_df = pd.melt(tell_stats, id_vars='Index', var_name='BA', value_name='TELL_Value')\n",
    "    tell_stats_df.rename(columns={'Index': 'Statistic'}, inplace=True)\n",
    "    \n",
    "    # Merge the two statistics dataframes together:\n",
    "    stats_df = pd.merge(gv_stats_df, tell_stats_df, on=['Statistic', 'BA'])\n",
    "    \n",
    "    # Drop the first row and last 4 rows:\n",
    "    tell_df = tell_df.tail(-1).iloc[:-4, :]\n",
    "    \n",
    "    # Compute the hourly total load across BAs:\n",
    "    tell_df['TELL_Total_Load_MWh'] = tell_df.sum(axis=1)\n",
    "    \n",
    "    # Copy the total load to a new dataframe:\n",
    "    tell_load_df = tell_df[['Index', 'TELL_Total_Load_MWh']].copy().iloc[0:8760, :]\n",
    "    tell_load_df.rename(columns={'Index': 'Hour'}, inplace=True)\n",
    "    \n",
    "    # Merge the two load dataframes together:\n",
    "    load_df = pd.merge(gv_load_df, tell_load_df, on=['Hour'])\n",
    "    \n",
    "    # Compute the min and max loads:\n",
    "    min_load = load_df[['GV_Total_Load_MWh','TELL_Total_Load_MWh']].min().min()\n",
    "    max_load = load_df[['GV_Total_Load_MWh','TELL_Total_Load_MWh']].max().max()\n",
    "    \n",
    "    # Subset the statistics dataframe:\n",
    "    avg_df = stats_df.loc[stats_df['Statistic'] == 'AVG']; avg_min = 0.95*avg_df[['GV_Value','TELL_Value']].min().min(); avg_max = 1.05*avg_df[['GV_Value','TELL_Value']].max().max()\n",
    "    sum_df = stats_df.loc[stats_df['Statistic'] == 'SUM']; sum_min = 0.95*sum_df[['GV_Value','TELL_Value']].min().min(); sum_max = 1.05*sum_df[['GV_Value','TELL_Value']].max().max()\n",
    "    min_df = stats_df.loc[stats_df['Statistic'] == 'MIN']; min_min = 0.95*min_df[['GV_Value','TELL_Value']].min().min(); min_max = 1.05*min_df[['GV_Value','TELL_Value']].max().max()\n",
    "    max_df = stats_df.loc[stats_df['Statistic'] == 'MAX']; max_min = 0.95*max_df[['GV_Value','TELL_Value']].min().min(); max_max = 1.05*max_df[['GV_Value','TELL_Value']].max().max()\n",
    "    \n",
    "    # Set the output filename:\n",
    "    if year_to_process == '2055':\n",
    "       load_filename = 'Load_Time_Series_2015_Weather_With_Climate_Change.png'\n",
    "       stats_filename = 'Load_Statistics_2015_Weather_With_Climate_Change.png'\n",
    "       plot_title_a = 'TELL Total Loads Based on 2015 Weather With Climate Change'\n",
    "    if year_to_process == '2058':\n",
    "       load_filename = 'Load_Time_Series_2018_Weather_With_Climate_Change.png'\n",
    "       stats_filename = 'Load_Statistics_2018_Weather_With_Climate_Change.png'\n",
    "       plot_title_a = 'TELL Total Loads Based on 2018 Weather With Climate Change'\n",
    "    \n",
    "    # Make the total load time series plots:\n",
    "    plt.figure(figsize=(24, 12))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    plt.subplot(211)\n",
    "    plt.plot(load_df['Hour'], load_df['GV_Total_Load_MWh'], color='black', linestyle='-', label='Original GridView Loads', linewidth=0.5)\n",
    "    plt.xlim([0, 8760]); plt.xticks([0, 8760],['0','8760']); plt.xlabel('')\n",
    "    plt.ylim([min_load, max_load]); plt.ylabel('WECC Total Load [MWh]')\n",
    "    plt.legend(loc='upper left', prop={'size': 12})\n",
    "    plt.title('Original Total Loads for 2035')\n",
    "        \n",
    "    plt.subplot(212)\n",
    "    plt.plot(load_df['Hour'], load_df['TELL_Total_Load_MWh'], color='black', linestyle='-', label='TELL Total Loads', linewidth=0.5)\n",
    "    plt.xlim([0, 8760]); plt.xticks([0, 8760],['0','8760']); plt.xlabel('Hour of Year [--->]')\n",
    "    plt.ylim([min_load, max_load]); plt.ylabel('WECC Total Load [MWh]') \n",
    "    plt.legend(loc='upper left', prop={'size': 12})\n",
    "    plt.title(plot_title_a)\n",
    "        \n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       plt.savefig(os.path.join(image_output_dir, load_filename), dpi=image_resolution, bbox_inches='tight', facecolor='white')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Make the statistics plots:\n",
    "    plt.figure(figsize=(24, 12))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.plot([avg_min, avg_max], [avg_min, avg_max], color='k', linestyle='-', linewidth=1)\n",
    "    plt.scatter(avg_df['GV_Value'], avg_df['TELL_Value'], s=50, c='blue')\n",
    "    plt.xlim([avg_min, avg_max]); plt.ylim([avg_min, avg_max]);\n",
    "    plt.xlabel('Original GridView Average Load [MWh]')\n",
    "    plt.ylabel('TELL Average Load [MWh]')\n",
    "    plt.title('Balancing Authority Annual Average Load')\n",
    "        \n",
    "    plt.subplot(222)\n",
    "    plt.plot([sum_min, sum_max], [sum_min, sum_max], color='k', linestyle='-', linewidth=1)\n",
    "    plt.scatter(sum_df['GV_Value'], sum_df['TELL_Value'], s=50, c='blue')\n",
    "    plt.xlim([sum_min, sum_max]); plt.ylim([sum_min, sum_max]);\n",
    "    plt.xlabel('Original GridView Annual Total Load [MWh]')\n",
    "    plt.ylabel('TELL Annual Total Load [MWh]')\n",
    "    plt.title('Balancing Authority Annual Total Load')\n",
    "        \n",
    "    plt.subplot(223)\n",
    "    plt.plot([min_min, min_max], [min_min, min_max], color='k', linestyle='-', linewidth=1)\n",
    "    plt.plot([min_min, min_max], [min_min, 0.9*min_max], color='k', linestyle='--', linewidth=1)\n",
    "    plt.plot([min_min, min_max], [min_min, 1.1*min_max], color='k', linestyle='--', linewidth=1)\n",
    "    plt.scatter(min_df['GV_Value'], min_df['TELL_Value'], s=50, c='blue')\n",
    "    plt.xlim([min_min, min_max]); plt.ylim([min_min, min_max]);\n",
    "    plt.xlabel('Original GridView Minimum Load [MWh]')\n",
    "    plt.ylabel('TELL Minimum Load [MWh]')\n",
    "    plt.title('Balancing Authority Annual Minimum Load')\n",
    "    \n",
    "    plt.subplot(224)\n",
    "    plt.plot([max_min, max_max], [max_min, max_max], color='k', linestyle='-', linewidth=1)\n",
    "    plt.plot([max_min, max_max], [max_min, 0.9*max_max], color='k', linestyle='--', linewidth=1)\n",
    "    plt.plot([max_min, max_max], [max_min, 1.1*max_max], color='k', linestyle='--', linewidth=1)\n",
    "    plt.scatter(max_df['GV_Value'], max_df['TELL_Value'], s=50, c='blue')\n",
    "    plt.xlim([max_min, max_max]); plt.ylim([max_min, max_max]);\n",
    "    plt.xlabel('Original GridView Maximum Load [MWh]')\n",
    "    plt.ylabel('TELL Maximum Load [MWh]')\n",
    "    plt.title('Balancing Authority Annual Maximum Load')\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.15, hspace=0.4)\n",
    "    \n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       plt.savefig(os.path.join(image_output_dir, stats_filename), dpi=image_resolution, bbox_inches='tight', facecolor='white')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abac0e8-534e-4599-aa2d-7d8e0489eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_summary_plots(data_dir = data_dir,\n",
    "                     year_to_process = '2058',\n",
    "                     image_output_dir = plot_dir, \n",
    "                     image_resolution = 300, \n",
    "                     save_images = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cfe36f-4a09-40eb-b7bb-94845448d878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.15_std",
   "language": "python",
   "name": "py3.9.15_std"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
